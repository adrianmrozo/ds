Project Report 2


---------
Task 1
---------

1.) We currently have 2 branches, that we treat as master file. The one called "main" is the default branch on github.
The other one called "master" contains the same information and needs to be set as default.
Repository Owner needs to follow the few simple steps in 
https://docs.github.com/en/free-pro-team@latest/github/administering-a-repository/changing-the-default-branch
CHECK

After we will have done that, we will delete the "main" branch.
CHECK

2.) Additionally there are two test branches. "testbranch" and "workon".
We want to delete the "testbranch" and keep "workon" as a testbranch.
CHECL

3.) Specification of the .gitignore file

All file types, respectively certain files within the local directory that are listed in the .gitignore file 
will be ignored when updating github to the status of the local directory. So we need to think about 
which files and filetypes we actually want to be displayed and shared on github. With our local directory 
growing (e.g. saving a trained CNN, adding text files with notes, ect.) and the premise to keep our respository 
nice and clean, we do not want all those newly added files to be uploaded into github.
Thus, with more and more files/file types to be stored locally and us not wanting those files to be shared, we have to 
update the .gitignore file on a regular basis.

In particular: 
- We ignore the our and all saved and trained CNN. We use: *.h5
- As specifically asked for in the instructions of this project, we want to avoid all pictures, video and music.
restict some file types representively. We use: for music: *.mp3, *.wav, *.flac 
                                                for videos: *.mp4, *.mov, *.wmv, *.avi, 
                                                for pictures: *.jpg, *.png, *.tiff, *.gif, *.raw

We already created a .gitignore file for Milestone 1. We copied the text and created via ```git touch``` a new 
textfile. We added some wildcards for additional file types. 
CHECK

Just upload the gitignore to workon.
CHECK BUT:
-------TODO
delete the folder and re-upload the gitignore and the project report. Don't know why that happened.



5.) Versioncontrol of the .gitignore - strategy
Use 4 branches within the repository: Master, workon_shared, user_1, user_2
Like this every team member can work on his own, seperated branch. Additionally, files that need to be shared
can be uploaded in the workon branch. Whenever files are being pushed into the master branch, a pull request 
needs to be confirmed by the other team member. 
Therefore we create the two new branches: workLK and workAM.
----TODO and to check with Adrian. OTHER IDEAS? 


6.) Prohibit direct uploads to the master branch: owner of the repository has to follow these steps: 
https://docs.github.com/en/free-pro-team@latest/github/administering-a-repository/enabling-required-status-checks
CHECK

However, now there are only unsupervised merges prohibited. One can still push data into the master branch. 
------TO DO

7.) Problems encountered: 
- Connecting the local directory to the online repository: In my understanding after cloning the repository to the directory there should all branches from the repository be also available locally. But when we check via git branch, we encounter only a master branch. From there we cannot add files to a different branch. 
Solution: Create a branch with a name like the branch you want to push files to.

- How to edit and upload a .gitignore file: Whenever I create the .txt file and rename it respectivly, it disappears. How can I load or push the file that doesn't get displayed?
Solution: Check the introduction video once more. Create the file not via a text editor but via ```touch .gitignore```. Make the file visible in the ubuntu folder with ctrl+H. Then the file can easily be edited and is also recognised by the git status command. Now it can be edited and then uploaded.


--------
Task 2
--------


1.) What is a hash function? For what is it being used? 

A hash function is a mathematical function that converts an input value (key, object, numerical, etc.) into another compressed numerical value (integer), the so called hash value. The input to the hash function is of arbitrary length but output is always of fixed length.

The hash values are often used as address keys for data inside an array. Thus search time for values can be decreased significantly (from O(log n) to O(1)).

Equal inputs receive the same hash value, whereas each two inequal inputs never get the same hash value.

Example: 
Book call number. In a library each book has a unique call number. The call number works like an address of the book within the library. If the same book is displayed multiple times they have the same call number, but to different books never have the same call number. 
With the book number one can easily navigate through the library and find the book much faster than checking every single book until the desired one is found. 



2.) Difference between python modules, packages and scripts

###Scripts:###
A python script is the code we can find in every python file, i.e. the entirety of all letters, words, numbers, variables, characters, functions, calculations, allocations, assignments, ect.
A script can be run directly in python, in Ipython or any python shell (e.g. Spyder or Google Colab).
A script saved as a python file is a module.


###Modules: ###
Any python file is a module. Its name is the file's base name without the ".py" ending. Consider a function we might have implemented in python and we name the file "test.py". Then we call this module "test".

It is considered as a best practice to split large Python code blocks into modules containing up to 300–400 lines of code.
This best practice is called "modularizing". It makes coding and development in python 
- easier : one only has to deal with a relatively small portion of the problem; coding is less error-prone
- maintainable: editing a single line in a large piece of code can cause errors due to interdepencies of the changed line. If a code is cut into modules that work seperately it decreases the likelihood that impacts of towards other parts of the code occur.
- reusable: Functionality defined in a single module can be easily reused
- scoping: Modules typically define a separate namespace, which helps avoid collisions between identifiers in different areas of a program.

###Package:###
A package is a collection of python modules or in more detail: This collection is structured as a dictionary with an additional _init_.py file. This _init_.py distinguishes a package from a directory.

When we import a package only the variables/functions/classes in the _init_.py file are visible. Subpackagages or modules are not. One needs to import those seperately. (compare importing keras and keras.datasets of our project code in python)

The distincion between module and package works only on the file system level. When a package or module is imported into python, they are both treated equally as type module. 

Depending on a Python application, one can consider to group the modules in sub-packages such as doc, core, utils, data, examples, test.

__init__.py is executed once when a module inside the package is referenced. This file can be left empty, or package-level initialization code can be implemented optionally.



3.) Explain DOCKER container and Volume

###Docker container:###
Docker is a platform that packages an application and all its dependencies together in the form of containers. This containerization aspect of Docker ensures that the application works in any environment. So to speak out of the box.

Containers are isolated from one another and bundle their own software, libraries and configuration files; they can communicate with each other through well-defined channels.

On Docker each and every application runs on separate containers and has its own set of dependencies and libraries. This makes sure that each application is independent of other applications, giving developers reassurance that they can build applications that will not interfere with one another.

###Docker volumes:###
Docker volumes are file systems mounted on Docker containers to preserve data generated by the running container.

The volumes are stored on the host, independent of the container life cycle. This allows users to back up data and share file systems between containers easily.


4.) Preference of using virtualenv vs. Docker. When would we use one or the other? 

We would use virtualenv when want to test something out on our local machines, prior to put it into operation, it is like our local workbench. However we can also use directly Docker, which is meant for the operational code.


5.) What is the Docker build context?

"The docker build command builds Docker images from a Dockerfile and a “context”. A build’s context is the set of files located in the specified PATH or URL. The build process can refer to any of the files in the context. For example, your build can use a COPY instruction to reference a file in the context."
Source: https://docs.docker.com/engine/reference/commandline/build/


6.) How can one assess the quality of a python package on PyPI?

By checking how many stars it has for example. Funny example, by accident one of the teammembers installed the package "paths" instead of the package "imutils". A misunderstanding after looking at the code line: 
'''from imutils import paths'''
After installing "paths" a weird error message shown. Comparing the two packages on PyPi:
imutils has 3348 stars (https://pypi.org/project/imutils/) while paths (https://pypi.org/project/paths/) has 0 stars. Said team member is slightly worried now.



--------
Task 3
--------- 

We successfully accomplished with our code already in milestone 1 the following tasks, respectively we know that the code has the following functionality:
- Can load data
- Can train (fit) a neural network on the data
- Can save a fitted model to a ".h5" file

Therefore we had to focus on the following two functionalities:
- Can load a ".h5" file, using Keras
- Can perform predictions using a "fitted" model, using Keras

Load a ".h5" file is very easy, we created a file (loadmodel.py) with this one python code line:
'''
model = load_model('keras_cifar10_trained_model.h5')
'''
However to do next with it, was significantly more challenging. Even though we found a command which should run the model on a given dataset, we had the feeling, that dataset part is challenging in this case, as a larger part of the cifar10 code seems to prepare the dataset. We found here a solution: 
https://gurus.pyimagesearch.com/lesson-sample-running-a-pre-trained-network/#

We tried their code by creating a test_network.py file with their code an executing the following command in the terminal:
'''
python3 test_network.py --model keras_cifar10_trained_model.h5 --test-images test_images
'''

Unfortunately in a first try/run the following error message appeared:
'''
  File "test_network.py", line 5, in <module>
    from imutils import paths
ModuleNotFoundError: No module named 'imutils'
'''

So we had to install the package imutils:
'''
pip3 install imutils
'''
After receiving another error message regarding a missing package, and after some research we also installed the opencv package:
'''
pip3 install opencv-python
'''
We tried to run the code again:
'''
python3 test_network.py --model keras_cifar10_trained_model.h5 --test-images test_images
'''
This time successfully, a bird picture popped up and the terminal it said the following:
'''
[INFO] sampling CIFAR-10...
[INFO] predicting on testing data...
[INFO] predicted: bird, actual: bird
[INFO] predicted: truck, actual: automobile
[INFO] predicted: frog, actual: dog
[INFO] predicted: ship, actual: ship
[INFO] predicted: horse, actual: truck
[INFO] predicted: cat, actual: cat
[INFO] predicted: ship, actual: ship
[INFO] predicted: truck, actual: truck
[INFO] predicted: airplane, actual: airplane
[INFO] predicted: truck, actual: automobile
[INFO] predicted: horse, actual: dog
[INFO] predicted: automobile, actual: automobile
[INFO] predicted: cat, actual: cat
[INFO] predicted: dog, actual: dog
[INFO] predicted: automobile, actual: truck
[INFO] testing on images NOT part of CIFAR-10

'''
On our machine all the above pictures were shown. We would also like to highlight the model was trained on another machine, and run with this python file on the machine of the other team member successfully. The images not part of CIFAR-10 were not shown anymore, we think it might be because the website actually asks for 95$ monthly so that one can download the code, so maybe the displayed code on the website was not the entire code. For our purposes to try to run the fitted model on another machine for a first time (even if it is just on testing data as written above), it was sufficient, we managed to fulfill:
- Can perform predictions using a "fitted" model, using Keras


--------
Task 4
--------- 

Modularization of the project code.

First we get to know the concept of modularization. The idea is to cut our project code in small sections that each one semantically independent. 
The whole code could be easily reassembled by linking the modules together again. However, we will have one "main.py" file where parts of the modules will be run in the respective order. 
After the modularization the code shoud run just like it did before with the important difference that the code will be found cut into snippets stored in several modules which are again all called at least once in the "main.py" file.


Procedure: 
As we already extensively commented the Code in the Milestone 1, it was easy to find lines on where to split semantically depending sections from each other. 
We found in a first attempt to modularize our Code 5 modules with their functions (in brackets):
- setInitials (setInitials)
- prep_cifar10 (prepareData, dataToCategorical)
- shapeModel (addModel, opt)
- training (train, augment)
- output (saveCNN, CNNstats)

The role of functions within the modules are self-explanitory as their names are well-chosen. 

Some of those modules contain 2 functions, some only one. 
By rewriting all code into functions that then can be used in the main.py file, we noticed the utmost importance of checking all variables that are being used in a particular function and to also declare them as inout variables for the function. 
Otherwise the function would never know, which values should be used. 

The modularised code has not yet been run in its new form.
TODO


--------- 
Task 5
--------- 

We installed successfully virtualenv with the below command as this was the first step in an instruction we found online (we encountered a challenge here, as the first instruction to install did not work, the command was "apt-get install python-virtualenv" and the following error message appeared: "E: Could not open lock file /var/lib/dpkg/lock-frontend - open (13: Permission denied) E: Unable to acquire the dpkg frontend lock (/var/lib/dpkg/lock-frontend), are you root?", a second instruction that we found suggested to use "sudo apt install python3-ven", as we did not want to use any sudo commands, we looked further and found the below command that worked):

'''
pip3 install virtualenv
'''

Next we executed the below command, according to the instruction the following happens once this command is executed: Creates a folder that houses the necessary Python executables in the bin directory. In this instance, we are installing Python 3.5 while also creating two folders, the virtualenvironment, and project_1 directory. Virtualenv will create the necessary directories in the project_1 directory. In this directory you’ll find bin, include, lib, local and share.

'''
virtualenv -p /usr/bin/python3 virtualenvironment/project_1
'''

Next moved to the bin folder:
'''
ls
cd virtualenvironment
cd project_1
cd bin
'''

With the following command we noticed as announced in the instruction a change in the terminal, every line started now with (project_1):
'''
source activate
'''

With the following command one can exit again the virtual environment:
'''
deactivate
'''

Next we decided to test this virtualenvironment out by cloning our files from our github repository with the following command:
'''
git clone https://github.com/adrianmrozo/ds.git
'''

Which worked. 

'''
git switch workon
'''

Next we tried to run the code, which should not work, as we did not yet install any packages in the virtual environment:
'''
python3 cifar10_cnn.py
'''

As anticipated, it did not work. Next we tried the following command to install the needed packages by using the requirements.txt file, as to an instruction we found online:
'''
pip install -r requirements.txt
'''
The installations were processed, however these error messages were shown:
'''
ERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.

We recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.

tensorflow 2.3.1 requires numpy<1.19.0,>=1.16.0, but you'll have numpy 1.19.2 which is incompatible.
'''
However at the end the following message was displayed:
'''
Successfully installed absl-py-0.11.0 astunparse-1.6.3 cachetools-4.1.1 certifi-2020.6.20 chardet-3.0.4 gast-0.3.3 google-auth-1.22.1 google-auth-oauthlib-0.4.2 google-pasta-0.2.0 grpcio-1.33.2 h5py-2.10.0 idna-2.10 keras-2.4.3 keras-preprocessing-1.1.2 markdown-3.3.3 numpy-1.19.2 oauthlib-3.1.0 opt-einsum-3.3.0 protobuf-3.13.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 pyyaml-5.3.1 requests-2.24.0 requests-oauthlib-1.3.0 rsa-4.6 scipy-1.5.3 six-1.15.0 tensorboard-2.3.0 tensorboard-plugin-wit-1.7.0 tensorflow-2.3.1 tensorflow-estimator-2.3.0 termcolor-1.1.0 urllib3-1.25.11 werkzeug-1.0.1 wrapt-1.12.1
'''
As it confirmed that keras 2.4.3 and tensorflow 2.3.1 were installed, we decided to try to run our code again:
'''
python3 cifar10_cnn.py
'''
It run successfully, it again started to train a model
'''
Epoch 1/100
 430/1563 [=======>......................] - ETA: 1:46 - loss: 2.0861 - accuracy: 0.2135
'''
Conclusion: We successfully created virtual environment. Our requirements.txt worked it downloaded exactly the versions as we specified in the requirements.txt file (to be verified if it was maybe just coincidence and these versions were maybe just the most up-to-date ones). And were able to run our code in the virtual environment with only 2 command lines. So we see the advantage of a requirements.txt file, to simplify an installation. We should at next opportunity, look into the error message, it looks like it would be better if we have a numpy version between 1.16.0 and 1.19.0, at a first sight, it appears that the easiest solution would be to define this also in the requirements file, so that the proper numpy version gets installed in a very first step.

Used sources/instructions: https://www.liquidweb.com/kb/creating-virtual-environment-ubuntu-16-04/





-----------
Task 6: 
-----------







-----------
Notes: 
-----------


How to create a new branch and push it to github
´´´
git branch <new_branch_name>
git checkout <new_branch_name>   # or both in one line $ git checkout -b <new_branch_name> 

git push --set-upstream origin <new_branch_name>

´´´

How to delete local branch
´´´
git branch -d <branch_name>
´´´

How to delete remote branch
´´´´
git push origin --delete remoteBranchName
´´´

How to push file into certain remote branch
``
git push origin <remote_branch_name>
``` 
 

How to pull default branch
´´´
git init
git pull <url_rep>
´´´


How to pull just a single branch from github: 
```
git clone <url_rep> --branch <branchname>
```

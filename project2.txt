Project Report 2


---------
Task 1
---------

1.) We currently have 2 branches, that we treat as master file. The one called "main" is the default branch on github.
The other one called "master" contains the same information and needs to be set as default.
Repository Owner needs to follow the few simple steps in 
https://docs.github.com/en/free-pro-team@latest/github/administering-a-repository/changing-the-default-branch
CHECK

After we will have done that, we will delete the "main" branch.
CHECK

2.) Additionally there are two test branches. "testbranch" and "workon".
We want to delete the "testbranch" and keep "workon" as a testbranch.
CHECL

3.) Specification of the .gitignore file

All file types, respectively certain files within the local directory that are listed in the .gitignore file 
will be ignored when updating github to the status of the local directory. So we need to think about 
which files and filetypes we actually want to be displayed and shared on github. With our local directory 
growing (e.g. saving a trained CNN, adding text files with notes, ect.) and the premise to keep our respository 
nice and clean, we do not want all those newly added files to be uploaded into github.
Thus, with more and more files/file types to be stored locally and us not wanting those files to be shared, we have to 
update the .gitignore file on a regular basis.

In particular: 
- We ignore the our and all saved and trained CNN. We use: *.h5
- As specifically asked for in the instructions of this project, we want to avoid all pictures, video and music.
restict some file types representively. We use: for music: *.mp3, *.wav, *.flac 
                                                for videos: *.mp4, *.mov, *.wmv, *.avi, 
                                                for pictures: *.jpg, *.png, *.tiff, *.gif, *.raw

We already created a .gitignore file for Milestone 1. We copied the text and created via ```git touch``` a new 
textfile. We added some wildcards for additional file types. 
CHECK

Just upload the gitignore to workon.
CHECK BUT:
-------TODO
delete the folder and re-upload the gitignore and the project report. Don't know why that happened.



5.) Versioncontrol of the .gitignore - strategy
Use 4 branches within the repository: Master, workon_shared, user_1, user_2
Like this every team member can work on his own, seperated branch. Additionally, files that need to be shared
can be uploaded in the workon branch. Whenever files are being pushed into the master branch, a pull request 
needs to be confirmed by the other team member. 
Therefore we create the two new branches: workLK and workAM.
----TODO and to check with Adrian. OTHER IDEAS? 


6.) Prohibit direct uploads to the master branch: owner of the repository has to follow these steps: 
https://docs.github.com/en/free-pro-team@latest/github/administering-a-repository/enabling-required-status-checks
CHECK

However, now there are only unsupervised merges prohibited. One can still push data into the master branch. 
------TO DO

7.) Problems encountered: 
- Connecting the local directory to the online repository: In my understanding after cloning the repository to the directory there should all branches from the repository be also available locally. But when we check via git branch, we encounter only a master branch. From there we cannot add files to a different branch. 
Solution: Create a branch with a name like the branch you want to push files to.

- How to edit and upload a .gitignore file: Whenever I create the .txt file and rename it respectivly, it disappears. How can I load or push the file that doesn't get displayed?
Solution: Check the introduction video once more. Create the file not via a text editor but via ```touch .gitignore```. Make the file visible in the ubuntu folder with ctrl+H. Then the file can easily be edited and is also recognised by the git status command. Now it can be edited and then uploaded.


--------
Task 2
--------



1.) What is a hash function? For what is it being used? 

2.) Difference between python modules, packages and scripts

3.) Explain DOCKER container and Volume

4.) Preference of using virtualenv vs. Docker. When would we use one or the other? 

We would use virtualenv when want to test something out on our local machines, prior to put it into operation, it is like our local workbench. However we can also use directly Docker, which is meant for the operational code.

5.) What is the Docker build contest?


6.) How can one assess the quality of a python package on PyPI?

By checking how many stars it has for example. Funny example, by accident one of the teammembers installed the package "paths" instead of the package "imutils". A misunderstanding after looking at the code line: 
'''from imutils import paths'''
After installing "paths" a weird error message shown. Comparing the two packages on PyPi:
imutils has 3348 stars (https://pypi.org/project/imutils/) while paths (https://pypi.org/project/paths/) has 0 stars. Said team member is slightly worried now.

--------
Task 3
--------- 

We successfully accomplished with our code already in milestone 1 the following tasks, respectively we know that the code has the following functionality:
- Can load data
- Can train (fit) a neural network on the data
- Can save a fitted model to a ".h5" file

Therefore we had to focus on the following two functionalities:
- Can load a ".h5" file, using Keras
- Can perform predictions using a "fitted" model, using Keras

Load a ".h5" file is very easy, we created a file (loadmodel.py) with this one python code line:
'''
model = load_model('keras_cifar10_trained_model.h5')
'''
However to do next with it, was significantly more challenging. Even though we found a command which should run the model on a given dataset, we had the feeling, that dataset part is challenging in this case, as a larger part of the cifar10 code seems to prepare the dataset. We found here a solution: 
https://gurus.pyimagesearch.com/lesson-sample-running-a-pre-trained-network/#

We tried their code by creating a test_network.py file with their code an executing the following command in the terminal:
'''
python3 test_network.py --model keras_cifar10_trained_model.h5 --test-images test_images
'''

Unfortunately in a first try/run the following error message appeared:
'''
  File "test_network.py", line 5, in <module>
    from imutils import paths
ModuleNotFoundError: No module named 'imutils'
'''

So we had to install the package imutils:
'''
pip3 install imutils
'''
After receiving another error message regarding a missing package, and after some research we also installed the opencv package:
'''
pip3 install opencv-python
'''
We tried to run the code again:
'''
python3 test_network.py --model keras_cifar10_trained_model.h5 --test-images test_images
'''
This time successfully, a bird picture popped up and the terminal it said the following:
'''
[INFO] sampling CIFAR-10...
[INFO] predicting on testing data...
[INFO] predicted: bird, actual: bird
[INFO] predicted: truck, actual: automobile
[INFO] predicted: frog, actual: dog
[INFO] predicted: ship, actual: ship
[INFO] predicted: horse, actual: truck
[INFO] predicted: cat, actual: cat
[INFO] predicted: ship, actual: ship
[INFO] predicted: truck, actual: truck
[INFO] predicted: airplane, actual: airplane
[INFO] predicted: truck, actual: automobile
[INFO] predicted: horse, actual: dog
[INFO] predicted: automobile, actual: automobile
[INFO] predicted: cat, actual: cat
[INFO] predicted: dog, actual: dog
[INFO] predicted: automobile, actual: truck
[INFO] testing on images NOT part of CIFAR-10

'''
On our machine all the above pictures were shown. We would also like to highlight the model was trained on another machine, and run with this python file on the machine of the other team member successfully. The images not part of CIFAR-10 were not shown anymore, we think it might be because the website actually asks for 95$ monthly so that one can download the code, so maybe the displayed code on the website was not the entire code. For our purposes to try to run the fitted model on another machine for a first time (even if it is just on testing data as written above), it was sufficient, we managed to fulfill:
- Can perform predictions using a "fitted" model, using Keras

--------- 
Task 5
--------- 

We installed successfully virtualenv with the below command as this was the first step in an instruction we found online (we encountered a challenge here, as the first instruction to install did not work, the command was "apt-get install python-virtualenv" and the following error message appeared: "E: Could not open lock file /var/lib/dpkg/lock-frontend - open (13: Permission denied) E: Unable to acquire the dpkg frontend lock (/var/lib/dpkg/lock-frontend), are you root?", a second instruction that we found suggested to use "sudo apt install python3-ven", as we did not want to use any sudo commands, we looked further and found the below command that worked):

'''
pip3 install virtualenv
'''

Next we executed the below command, according to the instruction the following happens once this command is executed: Creates a folder that houses the necessary Python executables in the bin directory. In this instance, we are installing Python 3.5 while also creating two folders, the virtualenvironment, and project_1 directory. Virtualenv will create the necessary directories in the project_1 directory. In this directory you’ll find bin, include, lib, local and share.

'''
virtualenv -p /usr/bin/python3 virtualenvironment/project_1
'''

Next moved to the bin folder:
'''
ls
cd virtualenvironment
cd project_1
cd bin
'''

With the following command we noticed as announced in the instruction a change in the terminal, every line started now with (project_1):
'''
source activate
'''

With the following command one can exit again the virtual environment:
'''
deactivate
'''

Next we decided to test this virtualenvironment out by cloning our files from our github repository with the following command:
'''
git clone https://github.com/adrianmrozo/ds.git
'''

Which worked. 

'''
git switch workon
'''

Next we tried to run the code, which should not work, as we did not yet install any packages in the virtual environment:
'''
python3 cifar10_cnn.py
'''

As anticipated, it did not work. Next we tried the following command to install the needed packages by using the requirements.txt file, as to an instruction we found online:
'''
pip install -r requirements.txt
'''
The installations were processed, however these error messages were shown:
'''
ERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.

We recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.

tensorflow 2.3.1 requires numpy<1.19.0,>=1.16.0, but you'll have numpy 1.19.2 which is incompatible.
'''
However at the end the following message was displayed:
'''
Successfully installed absl-py-0.11.0 astunparse-1.6.3 cachetools-4.1.1 certifi-2020.6.20 chardet-3.0.4 gast-0.3.3 google-auth-1.22.1 google-auth-oauthlib-0.4.2 google-pasta-0.2.0 grpcio-1.33.2 h5py-2.10.0 idna-2.10 keras-2.4.3 keras-preprocessing-1.1.2 markdown-3.3.3 numpy-1.19.2 oauthlib-3.1.0 opt-einsum-3.3.0 protobuf-3.13.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 pyyaml-5.3.1 requests-2.24.0 requests-oauthlib-1.3.0 rsa-4.6 scipy-1.5.3 six-1.15.0 tensorboard-2.3.0 tensorboard-plugin-wit-1.7.0 tensorflow-2.3.1 tensorflow-estimator-2.3.0 termcolor-1.1.0 urllib3-1.25.11 werkzeug-1.0.1 wrapt-1.12.1
'''
As it confirmed that keras 2.4.3 and tensorflow 2.3.1 were installed, we decided to try to run our code again:
'''
python3 cifar10_cnn.py
'''
It run successfully, it again started to train a model
'''
Epoch 1/100
 430/1563 [=======>......................] - ETA: 1:46 - loss: 2.0861 - accuracy: 0.2135
'''
Conclusion: We successfully created virtual environment. Our requirements.txt worked it downloaded exactly the versions as we specified in the requirements.txt file (to be verified if it was maybe just coincidence and these versions were maybe just the most up-to-date ones). And were able to run our code in the virtual environment with only 2 command lines. So we see the advantage of a requirements.txt file, to simplify an installation. We should at next opportunity, look into the error message, it looks like it would be better if we have a numpy version between 1.16.0 and 1.19.0, at a first sight, it appears that the easiest solution would be to define this also in the requirements file, so that the proper numpy version gets installed in a very first step.

Used sources/instructions: https://www.liquidweb.com/kb/creating-virtual-environment-ubuntu-16-04/

-----------
Notes: 
-----------


How to create a new branch and push it to github
´´´
git branch <new_branch_name>
git checkout <new_branch_name>   # or both in one line $ git checkout -b <new_branch_name> 

git push --set-upstream origin <new_branch_name>

´´´

How to delete local branch
´´´
git branch -d <branch_name>
´´´

How to delete remote branch
´´´´
git push origin --delete remoteBranchName
´´´

How to push file into certain remote branch
``
git push origin <remote_branch_name>
``` 
 

How to pull default branch
´´´
git init
git pull <url_rep>
´´´


How to pull just a single branch from github: 
```
git clone <url_rep> --branch <branchname>
```
